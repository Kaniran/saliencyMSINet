{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "history_visible": true,
      "authorship_tag": "ABX9TyP36WmFzBMPZvrkcR5oqEs/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kaniran/saliencyMSINet/blob/master/MasterDownload.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bUPaEwmAjKVs",
        "outputId": "e7910b4c-51d2-4b52-8f5f-3aebb33046cc"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.8/dist-packages (2.9.2)\n",
            "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.9.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.2.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (3.19.6)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow) (21.3)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from tensorflow) (57.4.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (14.0.6)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (4.4.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: tensorboard<2.10,>=2.9 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.9.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (0.29.0)\n",
            "Requirement already satisfied: flatbuffers<2,>=1.12 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.12)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.21.6)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.51.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.3.0)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.8/dist-packages (from astunparse>=1.6.0->tensorflow) (0.38.4)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.15.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.25.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->tensorflow) (3.0.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (5.2.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow) (6.0.0)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2022.12.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow) (3.11.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bpF1PUY8ka-E",
        "outputId": "2be90d02-ba25-4fef-cbd5-0df663020f50"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Jan 14 09:17:28 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   65C    P8    11W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BycQ2GxDjwoT",
        "outputId": "979ac95f-82fc-48b9-81e4-0afd9b3298e7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: tensorflow\n",
            "Version: 2.9.2\n",
            "Summary: TensorFlow is an open source machine learning framework for everyone.\n",
            "Home-page: https://www.tensorflow.org/\n",
            "Author: Google Inc.\n",
            "Author-email: packages@tensorflow.org\n",
            "License: Apache 2.0\n",
            "Location: /usr/local/lib/python3.8/dist-packages\n",
            "Requires: absl-py, astunparse, flatbuffers, gast, google-pasta, grpcio, h5py, keras, keras-preprocessing, libclang, numpy, opt-einsum, packaging, protobuf, setuptools, six, tensorboard, tensorflow-estimator, tensorflow-io-gcs-filesystem, termcolor, typing-extensions, wrapt\n",
            "Required-by: kapre\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUbpNXIWdgvD",
        "outputId": "4e74e853-4321-49b5-9884-54ec581a7d61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys \n",
        "import os\n",
        "\n",
        "sys.path.insert(0,'/content/drive/MyDrive/ColabNotebooks')\n",
        "\n",
        "from main import train_model, define_paths, test_1\n",
        "\n",
        "current_path = os.path.abspath(os.getcwd())\n",
        "\n",
        "results_path = current_path + \"/results/\"\n",
        "weights_path = current_path + \"/weights/\"\n",
        "\n",
        "history_path = results_path + \"history/\"\n",
        "images_path = results_path + \"images/\"\n",
        "ckpts_path = results_path + \"ckpts/\"\n",
        "\n",
        "best_path = ckpts_path + \"best/\"\n",
        "latest_path = ckpts_path + \"latest/\"\n",
        "\n",
        "data_path = \"/content/Data_saliency/\"\n",
        "\n",
        "paths = {\n",
        "        \"data\": data_path,\n",
        "        \"history\": history_path,\n",
        "        \"images\": images_path,\n",
        "        \"best\": best_path,\n",
        "        \"latest\": latest_path,\n",
        "        \"weights\": weights_path\n",
        "    }\n",
        "\n",
        "print(paths)\n",
        "\n",
        "a= test_1()\n",
        "print(a)\n",
        "train_model(\"salicon\",paths,\"gpu\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 678
        },
        "id": "JtxEvOb3hapS",
        "outputId": "45488640-f418-4a85-cb6d-99749a210401"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'data': '/content/Data_saliency/', 'history': '/content/results/history/', 'images': '/content/results/images/', 'best': '/content/results/ckpts/best/', 'latest': '/content/results/ckpts/latest/', 'weights': '/content/weights/'}\n",
            "test_2\n",
            "2\n",
            "Tesst\n",
            "Test\n",
            "SALICON\n",
            "Hello3\n",
            "Hello\n",
            ">> Downloading SALICON dataset...Access denied with the following error:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " \tCannot retrieve the public link of the file. You may need to change\n",
            "\tthe permission to 'Anyone with the link', or have had many accesses. \n",
            "\n",
            "You may still be able to access the file from the browser:\n",
            "\n",
            "\t https://drive.google.com/uc?id=1g8j-hTT-51IG1UFwP0xTGhLdgIUCW5e5&export=download \n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-e5525cce7c1f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mtest_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"salicon\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpaths\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"gpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/drive/MyDrive/ColabNotebooks/main.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(dataset, paths, device)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \"\"\"\n\u001b[1;32m     77\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Tesst\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m     \u001b[0miterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dataset_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpaths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"data\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0mnext_element\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_init_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_init_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/ColabNotebooks/data.py\u001b[0m in \u001b[0;36mget_dataset_iterator\u001b[0;34m(phase, dataset, data_path)\u001b[0m\n\u001b[1;32m    484\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Hello3\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m         \u001b[0mdataset_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    487\u001b[0m         \u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/ColabNotebooks/data.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_path)\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0mparent_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0mdownload\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_salicon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/ColabNotebooks/download.py\u001b[0m in \u001b[0;36mdownload_salicon\u001b[0;34m(data_path)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mgdown\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"tmp.zip\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquiet\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"tmp.zip\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mzip_ref\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip_ref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamelist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"test\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.8/zipfile.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps)\u001b[0m\n\u001b[1;32m   1249\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1250\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1251\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilemode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1252\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1253\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mfilemode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodeDict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/tmp.zip'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -la\n",
        "!pwd\n",
        "!ls -la ./drive\n",
        "!ls -la ./drive/MyDrive/ColabNotebooks/\n",
        "!cat ./drive/MyDrive/ColabNotebooks/main.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JnFj9VZsAsI0",
        "outputId": "821c92b9-67e9-49ce-999d-1de1f841289d"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 32\n",
            "drwxr-xr-x 1 root root 4096 Jan 14 11:09 .\n",
            "drwxr-xr-x 1 root root 4096 Jan 14 10:22 ..\n",
            "drwxr-xr-x 4 root root 4096 Jan  9 14:35 .config\n",
            "drwxr-xr-x 2 root root 4096 Jan 14 11:02 Data_saliency\n",
            "drwx------ 5 root root 4096 Jan 14 10:30 drive\n",
            "drwxr-xr-x 2 root root 4096 Jan 14 11:02 .ipynb_checkpoints\n",
            "drwxr-xr-x 1 root root 4096 Jan  9 14:36 sample_data\n",
            "drwxr-xr-x 2 root root 4096 Jan 14 11:09 Testfolder\n",
            "/content\n",
            "total 16\n",
            "dr-x------  2 root root 4096 Jan 14 10:30 .file-revisions-by-id\n",
            "drwx------ 18 root root 4096 Jan 14 10:30 MyDrive\n",
            "dr-x------  2 root root 4096 Jan 14 10:30 .shortcut-targets-by-id\n",
            "drwx------  5 root root 4096 Jan 14 10:30 .Trash-0\n",
            "total 153\n",
            "-rw------- 1 root root  1093 Jan 13 09:20 config.py\n",
            "-rw------- 1 root root 28495 Jan 14 11:17 data.py\n",
            "-rw------- 1 root root 16758 Jan 14 08:03 download.py\n",
            "drwx------ 2 root root  4096 Jan 13 09:27 .ipynb_checkpoints\n",
            "-rw------- 1 root root  1192 Jan 13 09:20 loss.py\n",
            "-rw------- 1 root root  8499 Jan 14 11:21 main.py\n",
            "-rw------- 1 root root 33652 Jan 13 09:29 Master_data.ipynb\n",
            "-rw------- 1 root root 16222 Jan 14 11:26 MasterDownload.ipynb\n",
            "-rw------- 1 root root 19194 Jan 14 10:35 model.py\n",
            "drwx------ 2 root root  4096 Jan 14 10:45 __pycache__\n",
            "-rw------- 1 root root 14702 Jan 13 09:30 Test.ipynb\n",
            "-rw------- 1 root root  5716 Jan 13 09:20 utils.py\n",
            "import argparse\n",
            "import os\n",
            "\n",
            "import numpy as np\n",
            "import tensorflow as tf\n",
            "\n",
            "import config\n",
            "import data\n",
            "import download\n",
            "import model\n",
            "import utils\n",
            "\n",
            "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
            "tf.get_logger().setLevel(\"ERROR\")\n",
            "\n",
            "\n",
            "def define_paths(current_path, args):\n",
            "    \"\"\"A helper function to define all relevant path elements for the\n",
            "       locations of data, weights, and the results from either training\n",
            "       or testing a model.\n",
            "\n",
            "    Args:\n",
            "        current_path (str): The absolute path string of this script.\n",
            "        args (object): A namespace object with values from command line.\n",
            "\n",
            "    Returns:\n",
            "        dict: A dictionary with all path elements.\n",
            "    \"\"\"\n",
            "\n",
            "    if os.path.isfile(args.path):\n",
            "        data_path = args.path\n",
            "    else:\n",
            "        data_path = os.path.join(args.path, \"\")\n",
            "\n",
            "    results_path = current_path + \"/results/\"\n",
            "    weights_path = current_path + \"/weights/\"\n",
            "\n",
            "    history_path = results_path + \"history/\"\n",
            "    images_path = results_path + \"images/\"\n",
            "    ckpts_path = results_path + \"ckpts/\"\n",
            "\n",
            "    best_path = ckpts_path + \"best/\"\n",
            "    latest_path = ckpts_path + \"latest/\"\n",
            "\n",
            "    if args.phase == \"train\":\n",
            "        if args.data not in data_path:\n",
            "            data_path += args.data + \"/\"\n",
            "\n",
            "    paths = {\n",
            "        \"data\": data_path,\n",
            "        \"history\": history_path,\n",
            "        \"images\": images_path,\n",
            "        \"best\": best_path,\n",
            "        \"latest\": latest_path,\n",
            "        \"weights\": weights_path\n",
            "    }\n",
            "\n",
            "    return paths\n",
            "\n",
            "def test_1():\n",
            "  print(\"test_2\")\n",
            "  return 2 \n",
            "\n",
            "\n",
            "def train_model(dataset, paths, device):\n",
            "    \"\"\"The main function for executing network training. It loads the specified\n",
            "       dataset iterator, saliency model, and helper classes. Training is then\n",
            "       performed in a new session by iterating over all batches for a number of\n",
            "       epochs. After validation on an independent set, the model is saved and\n",
            "       the training history is updated.\n",
            "\n",
            "    Args:\n",
            "        dataset (str): Denotes the dataset to be used during training.\n",
            "        paths (dict, str): A dictionary with all path elements.\n",
            "        device (str): Represents either \"cpu\" or \"gpu\".\n",
            "    \"\"\"\n",
            "    print(\"Tesst\")\n",
            "    iterator = data.get_dataset_iterator(\"train\", dataset, paths[\"data\"])\n",
            "\n",
            "    next_element, train_init_op, valid_init_op = iterator\n",
            "\n",
            "    input_images, ground_truths = next_element[:2]\n",
            "\n",
            "    input_plhd = tf.placeholder_with_default(input_images,\n",
            "                                             (None, None, None, 3),\n",
            "                                             name=\"input\")\n",
            "    msi_net = model.MSINET()\n",
            "\n",
            "    predicted_maps = msi_net.forward(input_plhd)\n",
            "\n",
            "    optimizer, loss = msi_net.train(ground_truths, predicted_maps,\n",
            "                                    config.PARAMS[\"learning_rate\"])\n",
            "\n",
            "    n_train_data = getattr(data, dataset.upper()).n_train\n",
            "    n_valid_data = getattr(data, dataset.upper()).n_valid\n",
            "\n",
            "    n_train_batches = int(np.ceil(n_train_data / config.PARAMS[\"batch_size\"]))\n",
            "    n_valid_batches = int(np.ceil(n_valid_data / config.PARAMS[\"batch_size\"]))\n",
            "\n",
            "    history = utils.History(n_train_batches,\n",
            "                            n_valid_batches,\n",
            "                            dataset,\n",
            "                            paths[\"history\"],\n",
            "                            device)\n",
            "\n",
            "    progbar = utils.Progbar(n_train_data,\n",
            "                            n_train_batches,\n",
            "                            config.PARAMS[\"batch_size\"],\n",
            "                            config.PARAMS[\"n_epochs\"],\n",
            "                            history.prior_epochs)\n",
            "\n",
            "    with tf.Session() as sess:\n",
            "        sess.run(tf.global_variables_initializer())\n",
            "        saver = msi_net.restore(sess, dataset, paths, device)\n",
            "\n",
            "        print(\">> Start training on %s...\" % dataset.upper())\n",
            "\n",
            "        for epoch in range(config.PARAMS[\"n_epochs\"]):\n",
            "            sess.run(train_init_op)\n",
            "\n",
            "            for batch in range(n_train_batches):\n",
            "                _, error = sess.run([optimizer, loss])\n",
            "\n",
            "                history.update_train_step(error)\n",
            "                progbar.update_train_step(batch)\n",
            "\n",
            "            sess.run(valid_init_op)\n",
            "\n",
            "            for batch in range(n_valid_batches):\n",
            "                error = sess.run(loss)\n",
            "\n",
            "                history.update_valid_step(error)\n",
            "                progbar.update_valid_step()\n",
            "\n",
            "            msi_net.save(saver, sess, dataset, paths[\"latest\"], device)\n",
            "\n",
            "            history.save_history()\n",
            "\n",
            "            progbar.write_summary(history.get_mean_train_error(),\n",
            "                                  history.get_mean_valid_error())\n",
            "\n",
            "            if history.valid_history[-1] == min(history.valid_history):\n",
            "                msi_net.save(saver, sess, dataset, paths[\"best\"], device)\n",
            "                msi_net.optimize(sess, dataset, paths[\"best\"], device)\n",
            "\n",
            "                print(\"\\tBest model!\", flush=True)\n",
            "\n",
            "\n",
            "def test_model(dataset, paths, device):\n",
            "    \"\"\"The main function for executing network testing. It loads the specified\n",
            "       dataset iterator and optimized saliency model. By default, when no model\n",
            "       checkpoint is found locally, the pretrained weights will be downloaded.\n",
            "       Testing only works for models trained on the same device as specified in\n",
            "       the config file.\n",
            "\n",
            "    Args:\n",
            "        dataset (str): Denotes the dataset that was used during training.\n",
            "        paths (dict, str): A dictionary with all path elements.\n",
            "        device (str): Represents either \"cpu\" or \"gpu\".\n",
            "    \"\"\"\n",
            "\n",
            "    iterator = data.get_dataset_iterator(\"test\", dataset, paths[\"data\"])\n",
            "\n",
            "    next_element, init_op = iterator\n",
            "\n",
            "    input_images, original_shape, file_path = next_element\n",
            "\n",
            "    graph_def = tf.GraphDef()\n",
            "\n",
            "    model_name = \"model_%s_%s.pb\" % (dataset, device)\n",
            "\n",
            "    if os.path.isfile(paths[\"best\"] + model_name):\n",
            "        with tf.gfile.Open(paths[\"best\"] + model_name, \"rb\") as file:\n",
            "            graph_def.ParseFromString(file.read())\n",
            "    else:\n",
            "        if not os.path.isfile(paths[\"weights\"] + model_name):\n",
            "            download.download_pretrained_weights(paths[\"weights\"],\n",
            "                                                 model_name[:-3])\n",
            "\n",
            "        with tf.gfile.Open(paths[\"weights\"] + model_name, \"rb\") as file:\n",
            "            graph_def.ParseFromString(file.read())\n",
            "\n",
            "    [predicted_maps] = tf.import_graph_def(graph_def,\n",
            "                                           input_map={\"input\": input_images},\n",
            "                                           return_elements=[\"output:0\"])\n",
            "\n",
            "    jpeg = data.postprocess_saliency_map(predicted_maps[0],\n",
            "                                         original_shape[0])\n",
            "\n",
            "    print(\">> Start testing with %s %s model...\" % (dataset.upper(), device))\n",
            "\n",
            "    with tf.Session() as sess:\n",
            "        sess.run(init_op)\n",
            "\n",
            "        while True:\n",
            "            try:\n",
            "                output_file, path = sess.run([jpeg, file_path])\n",
            "            except tf.errors.OutOfRangeError:\n",
            "                break\n",
            "\n",
            "            path = path[0][0].decode(\"utf-8\")\n",
            "\n",
            "            filename = os.path.basename(path)\n",
            "            filename = os.path.splitext(filename)[0]\n",
            "            filename += \".jpeg\"\n",
            "\n",
            "            os.makedirs(paths[\"images\"], exist_ok=True)\n",
            "\n",
            "            with open(paths[\"images\"] + filename, \"wb\") as file:\n",
            "                file.write(output_file)\n",
            "\n",
            "\n",
            "def main():\n",
            "    \"\"\"The main function reads the command line arguments, invokes the\n",
            "       creation of appropriate path variables, and starts the training\n",
            "       or testing procedure for a model.\n",
            "    \"\"\"\n",
            "\n",
            "    current_path = os.path.dirname(os.path.realpath(__file__))\n",
            "    default_data_path = current_path + \"/data\"\n",
            "\n",
            "    phases_list = [\"train\", \"test\"]\n",
            "\n",
            "    datasets_list = [\"salicon\", \"mit1003\", \"cat2000\",\n",
            "                     \"dutomron\", \"pascals\", \"osie\", \"fiwi\"]\n",
            "\n",
            "    parser = argparse.ArgumentParser(\n",
            "        formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
            "\n",
            "    parser.add_argument(\"phase\", metavar=\"PHASE\", choices=phases_list,\n",
            "                        help=\"sets the network phase (allowed: train or test)\")\n",
            "\n",
            "    parser.add_argument(\"-d\", \"--data\", metavar=\"DATA\",\n",
            "                        choices=datasets_list, default=datasets_list[0],\n",
            "                        help=\"define which dataset will be used for training \\\n",
            "                              or which trained model is used for testing\")\n",
            "\n",
            "    parser.add_argument(\"-p\", \"--path\", default=default_data_path,\n",
            "                        help=\"specify the path where training data will be \\\n",
            "                              downloaded to or test data is stored\")\n",
            "\n",
            "    args = parser.parse_args()\n",
            "\n",
            "    paths = define_paths(current_path, args)\n",
            "\n",
            "    if args.phase == \"train\":\n",
            "        train_model(args.data, paths, config.PARAMS[\"device\"])\n",
            "    elif args.phase == \"test\":\n",
            "        test_model(args.data, paths, config.PARAMS[\"device\"])\n",
            "\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    main()\n"
          ]
        }
      ]
    }
  ]
}